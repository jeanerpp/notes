* root
** 7510 syslog configure
   /etc/rsyslog.conf:
   # Include all config files in /etc/rsyslog.d/ 
   $IncludeConfig /etc/rsyslog.d/*.conf
   
   /etc/rsyslog.d/20-v7510.conf:
   redirect logs to /opt/v7510/data/SYSLOG.*
** NPU
*** 7510 NPU FIFO status
    def ui sys ena
    d test trace hds debug nfifd
*** HE NPU crash
    cased by integer printed as string: printf("%s",i)
    0x833f9a24 is at new_allocator.h:69.
    64      new_allocator.h: No such file or directory.
    in new_allocator.h
** log
*** journalctl -u network.service -x
*** check Linux service name
    chkconfig | grep -i openstack-cinder
    systemctl list-unit-files
**** logrotate
     logrotate may be used to split a file automatically for MPU.TRA
     it is defined in cron: /etc/cron.d/...
**** 7510 syslog
     [root@rms204-media-scm11 data]# grep "begin coredump" SYSLOG
     Dec 27 09:00:25 rms204-media-scm11 nodemgr: signal_handle_core_msg(): begin coredump: pid(2751)
     [root@rms204-media-scm11 data]# grep -w 2751 SYSLOG
     Dec 27 09:00:25 rms204-media-scm11 sshd[2751]: rexec line 57: Deprecated option RSAAuthentication
     Dec 27 09:00:25 rms204-media-scm11 sshd[2751]: rexec line 70: Deprecated option RhostsRSAAuthentication
     Dec 27 09:00:25 rms204-media-scm11 sshd[2751]: rexec line 124: Deprecated option UseLogin
     Dec 27 09:00:25 rms204-media-scm11 sshd[2751]: reprocess config line 57: Deprecated option RSAAuthentication
     Dec 27 09:00:25 rms204-media-scm11 sshd[2751]: reprocess config line 70: Deprecated option RhostsRSAAuthentication
     Dec 27 09:00:25 rms204-media-scm11 nodemgr: signal_handle_core_msg(): begin coredump: pid(2751)
     Dec 27 09:00:25 rms204-media-scm11 nodemgr: signal_handle_core_msg(): finish coredump: pid(2751),

** 7510
*** gfi dockor
    gfi doctor, may report CPU overload alarm, refer to:
    appl/apv/vm-apv-performance-alarm/apv_performance_alarm.c, pvm_tirgger_cpu_alarm()
*** iOAM login SCM
    ssh -o StrictHostKeyChecking=no diag@169.254.195.255
** ssh x11 forwarding
   Enable X11 over IPv4 from another host, configuration in /etc/ssh/sshd_config:
   X11Forwarding yes
   X11UseLocalhost no
   AddressFamily inet

   And install this package:
   yum install xorg-x11-xauth
   To solve problem like:
   X11 forwarding request failed on channel 0

   Because I change my HOME directory, I need to:
   export XAUTHORITY=/home/xujian/.Xauthority

   https://unix.stackexchange.com/questions/110558/su-with-error-x11-connection-rejected-because-of-wrong-authentication/118295
   record a way to enable x11 after su to root:
   To fix things, firstly detect which display number standarduser uses:

   standarduser@localhost:~$ echo $DISPLAY
   localhost:21.0

   In this case it is 21.0. Secondly, display standarduser's list of cookies:
   standarduser@localhost:~$ xauth list
   localhost/unix:1  MIT-MAGIC-COOKIE-1  51a3801fd7776704575752f09015c61d
   localhost/unix:21  MIT-MAGIC-COOKIE-1  0ba2913f8d9df0ee9eda295cad7b104f
   localhost/unix:22  MIT-MAGIC-COOKIE-1  22ba6595c270f20f6315c53e27958dfe
   localhost/unix:20  MIT-MAGIC-COOKIE-1  267f68b51726a8a381cfc10c91783a13
   The cookie for the 21.0 display is the second in the list and ends with 104f.
   
   The last thing to do is to add this particular cookie to the root's .Xauthority. Log in as root and do the following:
   root@localhost:~$ xauth add localhost/unix:21  MIT-MAGIC-COOKIE-1  0ba2913f8d9df0ee9eda295cad7b104
   This is a plain and simple how to deal with the X11 connection rejected because of wrong authentication error while you run su as different users in bash script or screen. And, by the way, thanks to this guy for his brilliant post.
   
   And an easier solution:
   
   1.- ssh user@host
   
   2.- $ sudo su
   
   3.- # xauth merge /home/user/.Xauthority
** ssh debug
   client: run with -v and -p 12345 option
   server: configure sshd in debug detail mode, or start another one with "/sbin/sshd -dd -D -p 12345"
** linux performance monitoring
   a long list: htop, dstat, collectl, nmon, saidar, sar, glances, atop
   http://www.binarytides.com/linux-system-monitoring-tools/
   http://collectl.sourceforge.net/Process.html
   http://www.brendangregg.com/blog/2015-07-08/choosing-a-linux-tracer.html
   http://elinux.org/Kernel_Trace_Systems
*** atop
    atop can see exited process status during two sample period. To show cpu running on: "s    Show scheduling characteristics"
    "atop -sy", we are able to see that top run and exit during this peroid
    PID       TID      ENVID      TRUN      TSLPI     TSLPU      POLI      NICE      PRI      RTPR     CPUNR      ST      EXC      S       CPU     CMD         1/1
    18120         -          -         0          0         0      -            -        -         -         -      NE        0      E        0%     <top>
*** perf
    use "-fno-omit-frame-pointer" in gcc to easy perf call graph analysis.
    
    howto make perf from new source:
    1 download from https://github.com/torvalds/linux
    2 in linux-master/tools/perf, run make
    
    refer to: http://www.brendangregg.com/perf.html
    record event and analyse detailed sched event:
    perf sched record -- sleep 10, or specify CPU(s), perf sched record --cpu 1 -- sleep 10
    perf script --header:
    swapper     0 [001] 65513.769647:       sched:sched_wakeup: xfsaild/dm-0:407 [120] success=1 CPU:001
    swapper     0 [001] 65513.769678:       sched:sched_switch: swapper/1:0 [120] R ==> xfsaild/dm-0:407 [120]

    this format is also find: perf record -e sched:sched_process_fork record bash

    record with call graph then display with "good" output:
    perf record -g fp|dwarf,4096 -t 8046 # if perf support dwarf format
    perf record -g -t 8046
    perf report --call-graph --stdio -G
*** trace-cmd
    trace-cmd record -e sched
    trace-cmd report:
    tuned-1609  [000] 65680.593381: sched_stat_runtime:   comm=tuned pid=1609 runtime=182320 [ns] vruntime=392716070185 [ns]
    tuned-1609  [000] 65680.593385: sched_switch:         tuned:1609 [120] S ==> swapper/0:0 [120]
*** top
    In "top" what are us, sy, ni, id, wa, hi, si and st (for CPU usage)?
    
    us - user cpu time (or) % CPU time spent in user space
    sy - system cpu time (or) % CPU time spent in kernel space
    ni - user nice cpu time (or) % CPU time spent on low priority processes
    id - idle cpu time (or) % CPU time spent idle
    wa - io wait cpu time (or) % CPU time spent in wait (on disk)
    hi - hardware irq (or) % CPU time spent servicing/handling hardware interrupts
    si - software irq (or) % CPU time spent servicing/handling software interrupts
    st - steal time % CPU time in involuntary wait by virtual cpu while hypervisor is servicing another processor (or) % CPU time stolen from a virtual machine

    if "wa" too high, process with state "D" should be checked:
    top - 20:07:11 up 36 min,  0 users,  load average: 7.32, 7.12, 6.39
    Tasks: 132 total,   2 running, 130 sleeping,   0 stopped,   0 zombie
    %Cpu0  :  1.5 us, 23.5 sy, 13.2 ni,  0.0 id, 51.5 wa,  0.0 hi,  0.0 si, 10.3 st
    4365 root      39  19  121336   4004   1068 D  15.0  0.0   0:01.68 mandb  0

    and related command: lsof, df, iotop (ref: http://bencane.com/2012/08/06/troubleshooting-high-io-wait-in-linux/)
*** memory usage in buffers/cache
    Meaning of the buffers/cache line in the output of top
    https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/5/html/Tuning_and_Optimizing_Red_Hat_Enterprise_Linux_for_Oracle_9i_and_10g_Databases/chap-Oracle_9i_and_10g_Tuning_Guide-Memory_Usage_and_Page_Cache.html
    Linux always tries to use RAM to speed up disk operations by using available memory for buffers (file system metadata) and cache (pages with actual contents of files or block devices)

    Meaning of the buffers/cache line in the output of free
    https://serverfault.com/questions/85470/meaning-of-the-buffers-cache-line-in-the-output-of-free
    A Linux system is really low on memory if the free value in -/+ buffers/cache: line gets low.
                 total      used      free   shared buffers    cached
    Mem:      12286456  11715372    571084        0   81912   6545228
    -/+ buffers/cache:   5088232   7198224
    Swap:     24571408     54528  24516880

    Use lsof to check all open files; use netstat to check pending bytes in socket. They will also consume buffers/cache.

    check memory used by kernel:
    https://unix.stackexchange.com/questions/97261/how-much-ram-does-the-kernel-use
    slabtop, grep Slab /proc/meminfo, /proc/slabinfo, /proc/modules, dmesg

    https://serverfault.com/questions/23433/in-linux-what-is-the-difference-between-buffers-and-cache-reported-by-the-f
    The "cached" total will also include some other memory allocations, such as any tmpfs filesytems. To see this in effect try:
    
    mkdir t
    mount -t tmpfs none t
    dd if=/dev/zero of=t/zero.file bs=10240 count=10240
    sync; echo 3 > /proc/sys/vm/drop_caches; free -m
    umount t
    sync; echo 3 > /proc/sys/vm/drop_caches; free -m
    and you will see the "cache" value drop by the 100Mb that you copied to the ram-based filesystem (assuming there was enough free RAM, you might find some of it ended up in swap if the machine is already over-committed in terms of memory use). The "sync; echo 3 > /proc/sys/vm/drop_caches" before each call to free should write anything pending in all write buffers (the sync) and clear all cached/buffered disk blocks from memory so free will only be reading other allocations in the "cached" value.
    
    The RAM used by virtual machines (such as those running under VMWare) may also be counted in free's "cached" value, as will RAM used by currently open memory-mapped files (this will vary depending on the hypervisor/version you are using and possibly between kernel versions too).
    
    So it isn't as simple as "buffers counts pending file/network writes and cached counts recently read/written blocks held in RAM to save future physical reads", though for most purposes this simpler description will do.
*** oprofile
    Refer to: http://oprofile.sourceforge.net/doc/opreport.html
    # setup
    [root@wirextorm1 xujian]# opcontrol --vmlinux=/boot/vmlinuz-2.6.32-431.el6.x86_64.wx1
    [root@wirextorm1 xujian]# opcontrol --callgraph=32
    [root@wirextorm1 xujian]# opcontrol --status
    Daemon not running
    Session-dir: /var/lib/oprofile
    Separate options: none
    vmlinux file: /boot/vmlinuz-2.6.32-431.el6.x86_64.wx1
    Image filter: none
    Call-graph depth: 20
    # running
    operf -p 20274 -g -t
    opreport --callgraph --demangle=normal
    opreport --symbols
    opannotate --source

** auditd and trace signal
*** watch file actions
    command setup: auditctl -w /home -p w
    configure setup in /etc/audit/auditd.conf: -w /home -p w
    check actions related to "/home/*", for example, /home/diag is deleted/created: ausearch -f /home -i
    ----
    type=PATH msg=audit(12/29/2016 13:01:38.854:187) : item=1 name=/home/diag inode=1310726 dev=fd:01 mode=dir,700 ouid=diag ogid=diag rdev=00:00 obj=unconfined_u:object_r:user_home_dir_t:s0 objtype=DELETE
    type=PATH msg=audit(12/29/2016 13:01:38.854:187) : item=0 name=/home/ inode=1179649 dev=fd:01 mode=dir,755 ouid=root ogid=root rdev=00:00 obj=unconfined_u:object_r:home_root_t:s0 objtype=PARENT
    type=CWD msg=audit(12/29/2016 13:01:38.854:187) :  cwd=/
    type=SYSCALL msg=audit(12/29/2016 13:01:38.854:187) : arch=x86_64 syscall=rmdir success=yes exit=0 a0=0x7f25702985e0 a1=0x0 a2=0xd1 a3=0x7f256e73a7b8 items=2 ppid=2491 pid=2695 auid=unset uid=root gid=root euid=root suid=root fsuid=root egid=root sgid=root fsgid=root tty=(none) ses=unset comm=userdel exe=/usr/sbin/userdel subj=system_u:system_r:unconfined_service_t:s0 key=(null)
    ----
    type=PATH msg=audit(12/29/2016 13:01:39.005:213) : item=1 name=/home/diag inode=1310726 dev=fd:01 mode=dir,000 ouid=root ogid=root rdev=00:00 obj=unconfined_u:object_r:user_home_dir_t:s0 objtype=CREATE
    type=PATH msg=audit(12/29/2016 13:01:39.005:213) : item=0 name=/home/ inode=1179649 dev=fd:01 mode=dir,755 ouid=root ogid=root rdev=00:00 obj=unconfined_u:object_r:home_root_t:s0 objtype=PARENT
    type=CWD msg=audit(12/29/2016 13:01:39.005:213) :  cwd=/
    type=SYSCALL msg=audit(12/29/2016 13:01:39.005:213) : arch=x86_64 syscall=mkdir success=yes exit=0 a0=0x7fff66f34f8d a1=0000 a2=0x7f19c907a778 a3=0x5f656d6f685f7265 items=2 ppid=2491 pid=2705 auid=unset uid=root gid=root euid=root suid=root fsuid=root egid=root sgid=root fsgid=root tty=(none) ses=unset comm=useradd exe=/usr/sbin/useradd subj=system_u:system_r:unconfined_service_t:s0 key=(null)
    ----
*** watch new process
    command setup: auditctl -a task,always
    check records: ausearch -i -sc execve
    ----
    type=PATH msg=audit(05/18/2017 20:45:27.125:2637) : item=1 name=/lib64/ld-linux-x86-64.so.2 inode=34343705 dev=fd:00 mode=file,755 ouid=root ogid=root rdev=00:00 obj=system_u:object_r:ld_so_t:s0 objtype=NORMAL
    type=PATH msg=audit(05/18/2017 20:45:27.125:2637) : item=0 name=/usr/bin/awk inode=16912380 dev=fd:00 mode=file,755 ouid=root ogid=root rdev=00:00 obj=system_u:object_r:bin_t:s0 objtype=NORMAL
    type=CWD msg=audit(05/18/2017 20:45:27.125:2637) :  cwd=/
    type=EXECVE msg=audit(05/18/2017 20:45:27.125:2637) : argc=3 a0=awk a1=/^(MemFree|Buffers|Cached):/ {free += $2}; END {print free} a2=/proc/meminfo
    type=SYSCALL msg=audit(05/18/2017 20:45:27.125:2637) : arch=x86_64 syscall=execve success=yes exit=0 a0=0x1556bf0 a1=0x15512b0 a2=0x1544ae0 a3=0x7ffea5b28430 items=2 ppid=12803 pid=12804 auid=unset uid=root gid=root euid=root suid=root fsuid=root egid=root sgid=root fsgid=root tty=(none) ses=unset comm=awk exe=/usr/bin/gawk subj=system_u:system_r:ksmtuned_t:s0 key=(null)
    ----
*** watch signals
    however it not works via following step according to https://www.ibm.com/developerworks/community/blogs/aimsupport/entry/Finding_the_source_of_signals_on_Linux_with_strace_auditd_or_Systemtap?lang=en
    auditctl -a exit,always -F arch=b64 -S kill -k kill_signals
    <--- do some kill action: start top and then kill it --->
    ausearch -i -k kill_signals, but find nothing. However detail things can be found in /var/log/audit/audit.log:
    type=OBJ_PID msg=audit(05/19/2017 00:54:31.993:1328072) : opid=18064 oauid=root ouid=root oses=26 obj=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 ocomm=top
    type=SYSCALL msg=audit(05/19/2017 00:54:31.993:1328072) : arch=x86_64 syscall=kill success=yes exit=0 a0=0x4690 a1=SIGKILL a2=0x0 a3=0x4690 items=0 ppid=12284 pid=12295 auid=root uid=root gid=root euid=root suid=root fsuid=root egid=root sgid=root fsgid=root tty=pts0 ses=19 comm=bash exe=/usr/bin/bash subj=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 key=kill_signals
*** perf signal
    perf record -a -e syscalls:sys_enter_kill sleep 60
    perf script:
                bash 17950 [001] 63637.957597: syscalls:sys_enter_kill: pid: 0x0000461e, sig: 0x00000009
*** systemtap
    seems to be fine, but little bit complicated since kernel debug symbol needed.
** RMS packet delay debug
*** perf command used
    in host: perf record sched --cpu "2-9,12-23,26-33,36-47"
    in vm: perf record sched --cpu "2-41"
    watch log: perf script --header
    perf record -e 'sched:sched_*' --cpu 40-41
    perf stat -e 'syscalls:sys_enter_*' -t 4424
    perf record -e sched:sched_switch --cpu 2-41
    perf record --event=sched:sched_switch --cpu "2-9,12-23,26-33,36-47"
*** bind host rcu_* kernel tasks to cpu 10
    from perf log, it is found that task switch between qemu-kvm and rcu_sched/rcu_bh:
    rcu_sched    10 [018] 511542.638867:       sched:sched_switch: rcu_sched:10 [120] S ==> qemu-kvm:14416 [120]
    qemu-kvm 14416 [018] 511542.641862:       sched:sched_wakeup: rcu_sched:10 [120] success=1 CPU:018
    qemu-kvm 14416 [018] 511542.642857:       sched:sched_switch: qemu-kvm:14416 [120] R ==> rcu_sched:10 [120]
    rcu_sched    10 [018] 511542.642866:       sched:sched_switch: rcu_sched:10 [120] S ==> qemu-kvm:14416 [120]
    qemu-kvm 14419 [043] 511542.644863:       sched:sched_wakeup: rcu_sched:10 [120] success=1 CPU:018
    qemu-kvm 14416 [018] 511542.645856:       sched:sched_switch: qemu-kvm:14416 [120] R ==> rcu_sched:10 [120]
    
    according to https://access.redhat.com/documentation/zh-CN/Red_Hat_Enterprise_Linux/7/html/Performance_Tuning_Guide/sect-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-CPU-Configuration_suggestions.html,
    it is suggested to bind rcu_* task to "slow speed" CPU, so it is done via command:
    $ for i in `pgrep rcu` ; do taskset -pc 10 $i ; done
*** bind vm rcu_* kernel tasks to cpu 0
    similar as host, done for vm:
    $ for i in `pgrep rcu` ; do taskset -pc 0 $i ; done
*** disable watchdog timer in vm
    from perf log, it is found that switch between npu dp task and watchdog:
    Ingress-14  4331 [020]  7320.190608:       sched:sched_wakeup: watchdog/20:106 [0] success=1 CPU:020
    Ingress-14  4331 [020]  7320.190615:       sched:sched_switch: Ingress-14:4331 [120] R ==> watchdog/20:106 [0]
    watchdog/20   106 [020]  7320.190620:       sched:sched_switch: watchdog/20:106 [0] S ==> Ingress-14:4331 [120]
    Ingress-14  4331 [020]  7320.911614: sched:sched_stat_runtime: comm=GfiLoop pid=4031 runtime=7572 [ns] vruntime=497839283800 [ns]
    Ingress-14  4331 [020]  7320.911617:       sched:sched_wakeup: rcu_sched:9 [120] success=1 CPU:000
    Ingress-14  4331 [020]  7324.190608:       sched:sched_wakeup: watchdog/20:106 [0] success=1 CPU:020
    
    then, it is decided to disable watchdog (manually at first for debug):
    $ sysctl kernel.nmi_watchdog=0
    and check this command run OK:
    $ cat /proc/sys/kernel/nmi_watchdog   # content should be 0

    then if use perf agagin, can not find switching between npu dp task and watchdog anymore.
*** swapless PIM vm kernel
    several days ago, I happened to find PIM vm kernel swap task sometimes run, can realy cost some CPU resources.
    I think this is unneccessary and disable it via command:
    $ echo 0 > /proc/sys/vm/swappiness
*** tickless PIM vm kernel
    timely tick may also be harmful to packet handling (refer to https://access.redhat.com/solutions/2273531), so disalbe it:
    - add command line parameter "nohz_full=2-41" to /etc/default/grub
      like: GRUB_CMDLINE_LINUX="default_hugepagesz=1G hugepagesz=1G hugepages=4 nohz_full=2-41 isolcpus=2-41 console=tty0 crashkernel=auto no_timer_check net.ifnames=0 console=ttyS0,115200n8"
    - run: grub2-mkconfig -o /boot/grub2/grub.cfg
    - reboot VM
*** disalbe all cron jobs in PIM vm (debug try only)
    command: systemctl stop crond
*** can find delay if too many sched switch events:
    "swapper 0" is system root process, it run when nothing to run.
    
    swapper 0 [008] Thu May 25 08:14:32 CST 2017.690073: sched:sched_switch: swapper
    /8:0 [120] R ==> Ingress-2:4399 [120]
    swapper 0 [016] Thu May 25 08:14:32 CST 2017.690080: sched:sched_switch: swapper
    /16:0 [120] R ==> Ingress-10:4407 [120]
    swapper 0 [018] Thu May 25 08:14:32 CST 2017.690082: sched:sched_switch: swapper
    /18:0 [120] R ==> Ingress-12:4409 [120]
    swapper 0 [024] Thu May 25 08:14:32 CST 2017.690089: sched:sched_switch: swapper
    /24:0 [120] R ==> Ingress-18:4415 [120]
    swapper 0 [032] Thu May 25 08:14:32 CST 2017.690092: sched:sched_switch: swapper
    /32:0 [120] R ==> Ingress-26:4423 [120]
    swapper 0 [015] Thu May 25 08:14:32 CST 2017.690094: sched:sched_switch: swapper
    /15:0 [120] R ==> Ingress-9:4406 [120]
    swapper 0 [017] Thu May 25 08:14:32 CST 2017.690097: sched:sched_switch: swapper
    /17:0 [120] R ==> Ingress-11:4408 [120]
    Ingress-6 4403 [012] Thu May 25 08:14:32 CST 2017.690629: sched:sched_switch: In
    gress-6:4403 [120] D ==> swapper/12:0 [120]
    Ingress-15 4412 [021] Thu May 25 08:14:32 CST 2017.690668: sched:sched_switch: I
    ngress-15:4412 [120] D ==> swapper/21:0 [120]
    Ingress-16 4413 [022] Thu May 25 08:14:32 CST 2017.690677: sched:sched_switch: I
    ngress-16:4413 [120] D ==> swapper/22:0 [120]
    Ingress-23 4420 [029] Thu May 25 08:14:32 CST 2017.690677: sched:sched_switch: I
    ngress-23:4420 [120] D ==> swapper/29:0 [120]
    Ingress-0 4397 [006] Thu May 25 08:14:32 CST 2017.690699: sched:sched_switch: In
    gress-0:4397 [120] D ==> swapper/6:0 [120]
    Ingress-25 4422 [031] Thu May 25 08:14:32 CST 2017.690705: sched:sched_switch: I
    ngress-25:4422 [120] D ==> swapper/31:0 [120]
    Ingress-17 4414 [023] Thu May 25 08:14:32 CST 2017.690730: sched:sched_switch: I
*** trace cache status
    http://www.brendangregg.com/blog/2014-12-31/linux-page-cache-hit-ratio.html
    https://github.com/brendangregg/perf-tools
    perf-tools-master.zip
    bin/cachestat -t 1
*** message ring (on 188 server)
**** collect message ring log
     fifo.tcl
**** check message ring log
     mgrep -d "d vi npu data statis ring" "RX-ring-process[0-9]+ +[0-9]+ +[0-9]+ +[0-9]{4}" fifo.0525.2 | more
*** other things may try
    1，观察dmesg
    2，log文件的时间戳
    3，syslog时间密度
    4，静态load
    5，关闭host任务：systemctl stop hp-snmp-agents
    6，连续抓sched
    7，numa观察，cache观察
    # cat /sys/devices/system/node/node0/numastat
    numa_hit 1055727933
    numa_miss 0
    numa_foreign 0
    interleave_hit 33310
    local_node 1055701026
    other_node 26907
    8，SCM上创建log
    9，更多的huge page
    10，删除pkg
    11，收集cache状态
** bash script
*** compare float point value: if [ $(echo "$cpu<5.0"|bc) -eq 1 ];then
*** disable stdio cache when pipe: function() < <(stdbuf -i0 -o0 -e0 top -b -d 0.1|stdbuf -i0 -o0 -e0 grep -w $selected_cpu)
*** date
**** convert epoch seconds to date: date --date='@1495610551'
**** convert date to epoch seconds: date -d "2017-05-25 18:43:13" +%s
**** convert current date to epoch seconds: date +%s
**** get system uptime in seconds: awk '{print $1}' /proc/uptime
*** force user to change password: chage -d0 limiaocai
*** process priority
    Use top seems to be the correct value.
    
    $ ps -eT -o pid,tid,comm,pri,ni
    1101  1366 docker-containe  19   0
    $ renice +19 -p 1366
    1101  1366 docker-containe   0  19
    $ ps -eT -o pid,tid,comm,pri,ni
    1101  1366 docker-containe   0  19
    $ top -b -H
    1366 root      39  19  205632   5808   3388 S  0.0  0.1   0:00.00 docker-containe
** trace Linux who change root password
   In selinux /var/log/audit logs:
   type=USER_CHAUTHTOK msg=audit(1495610551.948:360046): pid=22804 uid=0 auid=0 ses=38045 subj=unconfined_u:unconfined_r:passwd_t:s0-s0:c0.c1023 msg='op=PAM:chauthtok grantors=pam_pwquality,pam_unix acct="root" exe="/usr/bin/passwd" hostname=? addr=? terminal=pts/16 res=success'

   In "last" command:
   reboot   system boot  3.10.0-327.36.3. Thu May 25 02:05 - 06:51  (04:46)
   root     pts/13       172.24.142.99    Wed May 24 09:02 - 09:24  (00:21)
   root     pts/16       135.251.148.200  Wed May 24 07:22 - 07:22  (00:00)
   root     pts/15       172.24.142.39    Wed May 24 07:18 - 09:20  (02:01)
   root     pts/15       135.251.216.62   Wed May 24 07:08 - 07:08  (00:00)

   In /var/log/secure:
   May 24 07:18:43 sbc203-host01 sshd[16066]: pam_unix(sshd:session): session opened for user root by (uid=0)
   May 24 07:22:15 sbc203-host01 sshd[22784]: Accepted password for root from 135.251.148.200 port 51938 ssh2
   May 24 07:22:15 sbc203-host01 sshd[22784]: pam_unix(sshd:session): session opened for user root by (uid=0)
   May 24 07:22:31 sbc203-host01 passwd: pam_unix(passwd:chauthtok): password changed for root
   May 24 07:22:35 sbc203-host01 sshd[22784]: Received disconnect from 135.251.148.200: 11: disconnected by user
   May 24 07:22:35 sbc203-host01 sshd[22784]: pam_unix(sshd:session): session closed for user root
   May 24 07:23:29 sbc203-host01 unix_chkpwd[25268]: password check failed for user (root)
   May 24 07:23:29 sbc203-host01 sshd[25266]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=135.251.216.62  user=root

   and in ~/.bash_history can find command history.
** perf performance
   note the usage of annotate:
   Annotate loopa
     0.00 │      movl   $0x0,-0x4(%rbp)
     0.01 │    ↓ jmp    2b
     15.74│27:┌─→addl   $0x1,-0x4(%rbp)
     1.95 │2b:│  cmpl   $0x3e7,-0x4(%rbp)
     81.84│   └──jle    27
          │    #include <string.h>
          │    void loopa(int s)
          │    {
   then with gdb "disass /m loopa", we can find the source code corresponding to the offset 0x27(39) and 0x2b(43)
    12                  for(j=0;j<1000;j++);
      0x0000000000400492 <+30>:    movl   $0x0,-0x4(%rbp)
      0x0000000000400499 <+37>:    jmp    0x40049f <loopa+43>
      0x000000000040049b <+39>:    addl   $0x1,-0x4(%rbp)
      0x000000000040049f <+43>:    cmpl   $0x3e7,-0x4(%rbp)
      0x00000000004004a6 <+50>:    jle    0x40049b <loopa+39>
** gdb break and auto command
   $ cat gdbcmd
   break ckvg.c:16
   commands $bpnum
   print array[0]
   continue
   end
   
   attach 48510
   continue
   
   $ gdb ckvg
   Reading symbols from /home/xujian/tmp/ckvg...done.
   (gdb) source gdbcmd
   Breakpoint 1 at 0x4005ae: file ckvg.c, line 16.
   warning: process 48510 is a cloned process
   0x00000034204accc0 in __nanosleep_nocancel () from /lib64/libc.so.6
   
   Breakpoint 1, childFunc (arg=0x0) at ckvg.c:16
   16              sleep(1);
   $1 = 0 '\000'
   
   Breakpoint 1, childFunc (arg=0x0) at ckvg.c:16
   16              sleep(1);
   $2 = 0 '\000'
** Linux kni/namespace/team try
   ip addr add 192.168.48.1/24 dev br910
   nmtui
   nmcli con show "Team connection 1"
   805  ip link add link team1 name team1.10 type vlan id 10
   806  ip link add link team1 name team1.20 type vlan id 20
   808  ip netns add v10
   810  ip link set team1.10 netns v10
   811  ip netns exec v10 ip addr add 192.168.50.150/16 dev team1.10
   813  ip netns exec v10 ip link set team1.10 up
   814  ip netns exec v10 ip addr
   815  ip netns exec v10 ping 192.168.50.1
   
   9  dpdk_nic_bind.py --bind=igb_uio  0000:00:0b.0
   10  dpdk_nic_bind.py --bind=igb_uio  0000:00:0c.0
   11  dpdk_nic_bind.py --status
   start nodemgr
   ./kni -c 0fc0000000 -n 1 --proc-type primary -- -P -p 3 --config="(0,30,31,32),(1,33,34,35)"
   
   # ip link set vEth0_0 down
   APP: Configure network interface of 0 down
   # ip link set vEth1_0 down
   APP: Configure network interface of 1 down
   # teamd -g -f team.cfg -d
   Using team device "team1".
   Using PID file "/var/run/teamd/team1.pid"
   Using config file "/root/xujian/team.cfg"
   This program is not intended to be run as root.
   [ 3652.754752] team1: Mode changed to "activebackup"
   APP: Configure network interface of 0 up
   [ 3652.759912] team1: Port device vEth0_0 added
   APP: Configure network interface of 1 up
   [ 3652.762768] team1: Port device vEth1_0 added
   [root@sbc02-media-pim19 xujian]# teamdctl team1 state
   ----------------------
   [root@sbc02-media-pim19 xujian]# cat cfg.sh
   #!/bin/bash
   if ! ip link show team1;then
   echo ===============
   echo config team1...
   echo ===============
   ip link set vEth0_0 down
   ip link set vEth1_0 down
   teamd -g -f team.cfg -d
   ip link set team1 up
   ip link
   teamdctl team1 state
   fi
   
   for((i=$1;i<=$2;i++));do
   #echo =================
   #echo config vlan $i...
   #echo =================
   ns=v${i}
   ip link add link team1 name team1.$i type vlan id $i
   ip netns add $ns
   ip link set team1.$i netns $ns
   ip netns exec $ns ip addr add 11.11.12.250/24 dev team1.$i
   ip netns exec $ns ip link set team1.$i up
   #ip netns exec $ns ip addr
   done
   
   [root@sbc02-media-pim19 xujian]# cat dpdk.sh
   #!/bin/bash
   cd /root/xujian/target
   if ! [ -d /mnt/huge ]
   then
   echo ===================
   echo mount huge pages...
   echo ===================
   mkdir /mnt/huge
   mount -t hugetlbfs nodev /mnt/huge
   fi
   
   echo =================
   echo update dpdk ko...
   echo =================
   rmmod rte_kni
   rmmod igb_uio
   insmod igb_uio.ko
   insmod rte_kni.ko
   
   echo ========================
   echo bind 10GE nif to dpdk...
   echo ========================
   tools/dpdk-devbind.py --bind=igb_uio  0000:00:0b.0
   tools/dpdk-devbind.py --bind=igb_uio  0000:00:0c.0
   tools/dpdk-devbind.py --status
   
   echo ==========
   echo run kni...
   echo ==========
./kni -c 0fc0000000 -n 1 --proc-type primary -- -P -p 3 --config="(0,30,31,32),(1,33,34,35)"
** git
*** clone with proxy
    git clone --config "http.proxy=135.245.48.34:8000" https://github.com/sipcapture/homer-docker.git
    curl --proxy 135.245.48.34:8000 -L https://github.com/docker/compose/releases/download/1.6.2/run.sh > bin/docker-compose
    # set proxy
    git config --global http.proxy http://135.245.48.34:8000
    git config --global https.proxy http://135.245.48.34:8000
    # unset proxy
    git config --global --unset-all http.proxy
    git config --global --unset-all https.proxy
*** cherrypick
    git clone git@github.west.isilon.com:xjian/onefs.git
    git checkout remotes/origin/BR_BUG_PATCH_221280
    git banch -a
    git checkout -b BR_BUG_PATCH_221280
    git banch -v
    git branch -v
    git cherry-pick 1e0f3fc4386b10ae002d83a2712546989ec5a979
    git status
    git push origin BR_BUG_PATCH_221280
*** checkout remote branch as another branch
*** 
*** 
*** 
*** 
*** 
    git remote add ime-patch git@github.west.isilon.com:IME-Patch/Patches.git
    git remote -v
    git fetch ime-patch
    git branch -a
    git branch -v | more
    git branch -a | more
    git branch -a | grep 232710
    git checkout remotes/ime-patch/BR_BUG_PATCH_232710_PROD
    git checkout -b BR_BUG_PATCH_233180
    git log
    git push origin BR_BUG_PATCH_233180
*** fetch remote pr
    fetch PR head: git fetch origin +refs/pull/1/head:pr_1
    fetch PR merge: git fetch origin +refs/pull/1/merge:merge_1
    fetch All remotes: git fetch --tags --progress origin +refs/heads/*:refs/remotes/origin/* +refs/pull/*:refs/remotes/origin/pr/*
*** create centrolized repo
# on repo server
cd /home/xujian/gitorg/ita_ai
git --bare init --shared
# on dev client
git remote add origin xujian@10.84.120.183:/home/xujian/gitorg/ita_ai
git push --force origin HEAD
** check Linux bond interface status
   # cat /proc/net/bonding/bond3
   Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011)
   
   Bonding Mode: fault-tolerance (active-backup)
   Primary Slave: eno3 (primary_reselect always)
   Currently Active Slave: eno3
   MII Status: up
   MII Polling Interval (ms): 100
   Up Delay (ms): 0
   Down Delay (ms): 0
   
   Slave Interface: eno3
   MII Status: up
   Speed: 1000 Mbps
   Duplex: full
   Link Failure Count: 0
   Permanent HW addr: 1c:98:ec:1c:07:5a
   Slave queue ID: 0
   
   Slave Interface: eno4
   MII Status: up
   Speed: 1000 Mbps
   Duplex: full
   Link Failure Count: 0
   Permanent HW addr: 1c:98:ec:1c:07:5b
   Slave queue ID: 0
** check host/vm network device mapping
   $ virsh dumpxml 562
   ...
    <interface type='bridge'>
      <mac address='82:01:20:83:01:01'/>
      <source bridge='br910'/>
      <target dev='vnet16'/>
      <model type='virtio'/>
      <filterref filter='mac-filter'/>
      <alias name='net1'/>
      <rom bar='on' file='/opt/vcp/share/ipxe/virtio-net.rom'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </interface>
   # in vm:
   $ ethtool -i eth0
    driver: virtio_net
    version: 1.0.0
    firmware-version:
    expansion-rom-version:
    bus-info: 0000:00:04.0
    supports-statistics: no
    supports-test: no
    supports-eeprom-access: no
    supports-register-dump: no
    supports-priv-flags: no
   # can see the pci slot both is 4, and the host device is vnet16, vm device is eth0
   # the MAC addresses are also match
** account backup
*** redhat
    username: qingbma
    password: A7510team6
    register redhat via these account with command subscription-manager-gui, and proper http proxy set.
*** windriver
    Web :  http://support.windriver.com/.
    Usr   : Binda.Wang@alcatel-sbell.com.cn
    Pwd : 4rfv$RFV
** kvm/docker on Linux
   On Redhat 6.5 server (188):
   ================================
   modprobe kvm
   modprobe kvm_intel
   setenforce 0
   . myproxy
   yum install qemu-kvm qemu-img
   yum install virt-manager libvirt libvirt-python libvirt-client
   service libvirtd start
   service libvirtd status
   virt-manager

   In VM:
   ================================
   # enable eth0
   vi /etc/sysconfig/network-scripts/ifcfg-eth0
   systemctl restart network
   # set http/https proxy
   export http_proxy="http://135.245.48.34:8000"
   export https_proxy="http://135.245.48.34:8000"
   # configure dns
   vi /etc/resolv.conf
   # x11 display
   export DISPLAY=172.24.142.171:0.0
   # update packages
   subscription-manager register
   # remove attached pools
   subscription-manager remove --all
   # Service Level:       Self-Support from `subscription-manager list --available`
   subscription-manager attach --pool=8a85f9895b465083015b46d2acd62df4
   subscription-manager attach --auto  # may not work
   yum update

   # https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html/getting_started_with_containers/get_started_with_docker_formatted_container_images
   # for docker, additional confiugre in vm
   # .bashrc
   export http_proxy="http://135.245.48.34:8000"
   export https_proxy="http://135.245.48.34:8000"
   export HTTP_PROXY="http://135.245.48.34:8000"
   export HTTPS_PROXY="http://135.245.48.34:8000"
   # /etc/sysconfig/docker
   http_proxy="http://135.245.48.34:8000"
   https_proxy="http://135.245.48.34:8000"
   HTTP_PROXY="http://135.245.48.34:8000"
   HTTPS_PROXY="http://135.245.48.34:8000"
   
   # from Gesciak, Krzysztof: VM proxy for docker
   [root@epos02 docker]# cat /etc/systemd/system/docker.service.d/http-proxy.conf
   [Service]
   Environment="HTTP_PROXY=http://proxy.lbs.alcatel-lucent.com:8000/" "HTTPS_PROXY=http://proxy.lbs.alcatel-lucent.com:8000/" "NO_PROXY=localhost,registry.epos.local"
   
   [root@epos02 docker]# cat /etc/resolv.conf
   options single-request
   nameserver 10.2.11.3
   nameserver 172.23.102.60
   nameserver 135.239.25.18

   In docker
   ================================
   # from Gesciak, Krzysztof: proxy config In docker
   /etc/systemd/system/docker.service.d/http-proxy.conf is only applicable for docker service.
   
   So, it is used by docker to load image (docker search, docker pull).
   When you start container, proxy setting is not passed into container.
   
   If you want to use it, you can specify http_proxy in your dockerfile
   
   Here is an example of dockerfile from Rapport: http://epos01.pl-lab.lucent.com:9000/cgit.cgi/EPOS_plat/tree/dockerfiles/Dockerfile.epos-doc
   
   ENV proxy=proxy.lbs.alcatel-lucent.com
   ENV http_proxy=${http_proxy:-http://$proxy:8000}
   ENV https_proxy=${https_proxy:-https://$proxy:8000}
   
   In that case entry proxy.lbs.alcatel-lucent.com must be resolved, you can use IP here of course.
   To verify, run ‘docker run -ti ‘your image’ /bin/bash’ and manually execute your commands (wget …) to check if it works.
*** enable normal user to access docker service
    usermod -aG docker xujian
*** resovle "error creating vxlan interface file exists docker" (journalctl -u docker.service)
    - docker stack rm
    - docker swarm leave --force
    - docker network rm ...
    - systemctl stop docker
    - umount /var/run/docker/netns/*; /bin/rm -f /var/run/docker/netns/*
    - be careful to use "ip link delete ...", make sure links are of docker space
    13: docker_gwbridge: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP 
    link/ether 02:42:b6:cf:14:cc brd ff:ff:ff:ff:ff:ff
    inet 172.18.0.1/16 scope global docker_gwbridge
    valid_lft forever preferred_lft forever
    inet6 fe80::42:b6ff:fecf:14cc/64 scope link 
    valid_lft forever preferred_lft forever
    15: vetha88c1b0@if14: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker_gwbridge state UP 
    link/ether 52:7b:50:0f:30:55 brd ff:ff:ff:ff:ff:ff link-netnsid 1
    inet6 fe80::507b:50ff:fe0f:3055/64 scope link 
    valid_lft forever preferred_lft forever
    19: vethb806235@if18: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker_gwbridge state UP 
    link/ether de:72:2e:2b:0b:24 brd ff:ff:ff:ff:ff:ff link-netnsid 3
    inet6 fe80::dc72:2eff:fe2b:b24/64 scope link 
    valid_lft forever preferred_lft forever
    26: veth0104cde@if25: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker_gwbridge state UP 
    link/ether 96:90:09:61:19:65 brd ff:ff:ff:ff:ff:ff link-netnsid 4
    inet6 fe80::9490:9ff:fe61:1965/64 scope link 
    valid_lft forever preferred_lft forever
    - restart machine
    - systemctl start docker
    - docker swarm init
    - rename docker stack network in yml config
    - docker deploy
*** volume backup:
    https://hub.docker.com/p/loomchild/volume-backup
*** upgrade to docker-ce to support more features such as nfs mount
refer to https://docs.docker.com/install/linux/docker-ce/centos/, https://docs.docker.com/compose/compose-file/#external

** substitute '\n' with newline: sed 's/\\n/\n/g'
** kernel task backtrace when has 'D' state in atop
   echo w > /proc/sysrq-trigger
   then:
   dmesg
** ip tools
*** socat for proxy
    use nc to send/receive packets
    udp proxy:
    socat UDP4-LISTEN:30000,fork UDP4-SENDTO:135.251.216.188:40000
    nc -u 127.0.0.1 30000

    tcp proxy:
    socat TCP-LISTEN:30000,fork TCP:135.251.216.188:40000
    nc 127.0.0.1 30000
*** iptables
    snat:
    iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE

    dnat:
    if FORWARD chain policy is drop: iptables -A FORWARD -i eth0 -p tcp --dport 80 -d 172.31.0.23 -j ACCEPT
    iptables -t nat -A PREROUTING -i eth0 -p tcp --dport 80 -j DNAT --to 172.31.0.23:80
    examples of iptables for DMZ NAT:
    https://www.cs.montana.edu/courses/309/topics/nat/dmz.html

    mangle:
    iptables -t mangle -A FORWARD -p tcp --tcp-flags SYN,RST SYN -j TCPMSS --set-mss 1452
    http://andys.org.uk/bits/2010/01/27/iptables-fun-with-mark/
    iptables -t mangle -A PREROUTING -p tcp --dport 22 -j MARK --set-mark 2
    iptables -A PREROUTING -m mark --mark 0xE -j ACCEPT

    display rules:
    "iptables -v -L" shows rules on interface:
    0     0 ACCEPT     all  --  lo     any     anywhere             anywhere
    0     0 REJECT     all  --  any    any     anywhere             anywhere             reject-with icmp-host-prohibited        
*** get linux pmtu
    $ ip route get to 10.11.14.102
    10.11.14.102 dev eth1  src 10.11.14.13
    cache  mtu 1500 advmss 1460 hoplimit 64
    $ ip route get to 10.11.14.102 from 10.11.14.13 dev eth1
    10.11.14.102 from 10.11.14.13 dev eth1
    cache  mtu 1500 advmss 1460 hoplimit 64
** Linux bootable usb
   - plugin usb and mount it:
     fdisk -l: /dev/sdb4   *          63   121063424    60531681    c  W95 FAT32 (LBA)
     mount -t vfat /dev/sdb4 /root/usb
   - on server 135.251.217.120 (root/;x7510j), run /home/xujian/bin/unetbootin-linux64-657.bin
   - use GUI to "burn" iso into usb
   - umount /root/usb
     ^^^^
     however unetbootin can not work!!! use this way on verion >6.5
   - mkfs.vfat -n 'centos7' -I /dev/sdb4
   - dd if=./iso/CentOS-7-x86_64-DVD-1511.iso of=/dev/sdb
   - sync && eject /dev/sdb
** ps thread in my favorite way
   $ ps -C wx-main -T -o pid,tid,class,psr,comm,cmd
   52631 52631 TS   38 wx-main         wirextorm 2960
   52631 52633 TS   37 wx-sctp-iterato wirextorm 2960
   52631 52634 TS    0 wx-sctp-timer   wirextorm 2960
   52631 52635 TS   38 wx-sending-0    wirextorm 2960
   52631 52636 TS    0 wx-polling-0    wirextorm 2960
** openvpn config
   - install packages
   yum install epel-release
   yum -y install openvpn easy-rsa iptables-services

   - generate keys
   cp -a /usr/share/easy-rsa /etc/openvpn/
   cd /etc/openvpn/easy-rsa/2.*/
   vi vars
   source ./vars
   ./clean-all
   ./build-ca
   ./build-key-server server
   ./build-dh
   ./build-key client
   cp -r keys/ /etc/openvpn/
   cd /etc/openvpn/
   openvpn --genkey --secret ta.key

   - config: client.ovpn and server.conf
   vi server.conf
   mkdir /etc/openvpn/log
   vi /etc/sysctl.conf: net.ipv4.ip_forward = 1

   /cygdrive/c/Users/jixu/OpenVPN/config $ ls  # client key copy from server easy-rsa
   ca.crt  client.crt  client.key  client.ovpn  ta.key
   
   [root@sbca log]# ls /etc/openvpn
   client  easy-rsa  keys  log  server  server.conf

   - iptables rules
   iptables -I INPUT -p udp --dport 4891 -j ACCEPT
   iptables -I INPUT -p tcp --dport 4891 -j ACCEPT
   iptables -t nat -I POSTROUTING -s 192.168.250.0/24 -o eno1 -j MASQUERADE
   iptables -I FORWARD -p tcp -s 192.168.250.0/24 -j ACCEPT
   iptables -I FORWARD -p udp -s 192.168.250.0/24 -j ACCEPT

   - start server
   openvpn --config /etc/openvpn/server.conf &

   - with proxy
     tcp mode
     client proxy:
     socat TCP-LISTEN:10000,fork TCP:135.251.217.191:10000
     server proxy:
     socat TCP-LISTEN:10000,fork TCP:127.0.0.1:4891
     and don't forget enable proxy port:
     iptables -I INPUT -p tcp --dport 10000 -j ACCEPT
     iptables -I INPUT -p udp --dport 10000 -j ACCEPT
** yum 
*** disable yum ipv6 repo
    disable ipv6:
    sysctl -w net.ipv6.conf.all.disable_ipv6=1
    sysctl -w net.ipv6.conf.default.disable_ipv6=1

    then:
    systemctl restart network

    then:
    However you still try to contact IPv6 addresses, so the solution consists in adding this line
      ip_resolve=4
    to
      /etc/yum.conf
*** specific yum repo
    yum install --disablerepo="*" --enablerepo="7_x86_64_*" procmail
** openssl
   show server cert: openssl s_client -connect testdb-efrm.sea1.apps.isilon.com:443 -showcerts
   read cert: openssl x509 -in /home/xujian/test/client-cert.pem -noout -text
   read key : openssl rsa -in /home/xujian/test/client-key.pem -noout -text
   cert fingerprint: openssl x509 -in /home/xujian/test/client-cert.pem -noout -sha256 -fingerprint
   read ca bundle: openssl crl2pkcs7 -nocrl -certfile cacert.pem | openssl pkcs7 -print_certs -noout
   generate self signed key/cert: openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -days 365 -nodes -subj '/CN=10.32.130.31:5000'
     -nodes: no key password protection
     -subj: https host FQDN name, -subj "/C=US/ST=Oregon/L=Portland/O=Company Name/OU=Org/CN=www.example.com"
** download/extract rpm package
   repoquery --show-duplicates kernel-headers-3.10.0
   yumdownloader kernel-headers-0:3.10.0-514.2.2.el7.x86_64
   rpm2cpio ../kernel-headers-3.10.0-514.2.2.el7.x86_64.rpm | cpio -idmv

   # tar install rpm files
   rpm -q -l tcpdump | tar -czf tcpdump.tgz --files-from -
** pv, vg, and lv
   https://www.digitalocean.com/community/tutorials/an-introduction-to-lvm-concepts-terminology-and-operations
** virtualization
   modify qcow2 root password, use "libguestfs-tools-c".
   LIBGUESTFS_BACKEND=direct virt-customize -a CentOS-7-x86_64-GenericCloud-1711.qcow2 --root-password password:abc123

   Manualy create qcow2 from iso: https://docs.openstack.org/image-guide/centos-image.html

   Modify qcow2: https://docs.openstack.org/image-guide/modify-images.html, http://libguestfs.org/guestfs-recipes.1.html
   LIBGUESTFS_BACKEND=direct guestmount -a CentOS-7-x86_64-GenericCloud-1711.qcow2 -i --rw /mnt
   chroot /mnt
   # modify something
   umount /mnt
** eclipse
   use behind http proxy, in eclipse.ini (directory where eclipse installed):
   -Djava.net.useSystemProxies=true
   -Dhttp.proxyHost=135.245.48.34
   -Dhttp.proxyPort=8000
   -Dhttps.proxyHost=135.245.48.34
   -Dhttps.proxyPort=8000
   -DsocksProxyHost=135.245.48.34
   -DsocksProxyPort=8000
   -Dftp.proxyHost=135.245.48.34
   -Dftp.proxyPort=8000
   -Dhttp.nonProxyHosts=localhost|127.0.0.1

   In Eclipse, Window > Preferences > General > Network Connections, set Active Provider = Native.

   For maven to use proxy, edit C:\Users\{username}\.m2\settings, then set in Maven → User Settings.
   Refer to http://www.javahelps.com/2015/08/set-proxy-for-maven-in-eclipse.html, https://stackoverflow.com/questions/7737710/maven-plugin-not-using-eclipses-proxy-settings.

   quick start maven in Linux: https://maven.apache.org/guides/getting-started/maven-in-five-minutes.html
** use hash to generate passwd:
   $ openssl sha -sha1 -hex <<< 201802 | cut -d' ' -f 2 | cut -c -8
   ec0eae8c
   Then: ;xec0eae8cj02
** convert doc to pdf format
   unoconv mydoc.pptx
   
** how to query all repo in enterprise github
   xujian@EAT ~/tmp/fix $ curl -X GET -u xjian https://github.west.isilon.com/api/v3
   Enter host password for user 'xjian':
   {
   "current_user_url": "https://github.west.isilon.com/api/v3/user",
   "current_user_authorizations_html_url": "https://github.west.isilon.com/settings/connections/applications{/client_id}",
   "authorizations_url": "https://github.west.isilon.com/api/v3/authorizations",
   "code_search_url": "https://github.west.isilon.com/api/v3/search/code?q={query}{&page,per_page,sort,order}",
   "commit_search_url": "https://github.west.isilon.com/api/v3/search/commits?q={query}{&page,per_page,sort,order}",
   "emails_url": "https://github.west.isilon.com/api/v3/user/emails",
   "emojis_url": "https://github.west.isilon.com/api/v3/emojis",
   "events_url": "https://github.west.isilon.com/api/v3/events",
   "feeds_url": "https://github.west.isilon.com/api/v3/feeds",
   "followers_url": "https://github.west.isilon.com/api/v3/user/followers",
   "following_url": "https://github.west.isilon.com/api/v3/user/following{/target}",
   "gists_url": "https://github.west.isilon.com/api/v3/gists{/gist_id}",
   "hub_url": "https://github.west.isilon.com/api/v3/hub",
   "issue_search_url": "https://github.west.isilon.com/api/v3/search/issues?q={query}{&page,per_page,sort,order}",
   "issues_url": "https://github.west.isilon.com/api/v3/issues",
   "keys_url": "https://github.west.isilon.com/api/v3/user/keys",
   "notifications_url": "https://github.west.isilon.com/api/v3/notifications",
   "organization_repositories_url": "https://github.west.isilon.com/api/v3/orgs/{org}/repos{?type,page,per_page,sort}",
   "organization_url": "https://github.west.isilon.com/api/v3/orgs/{org}",
   "public_gists_url": "https://github.west.isilon.com/api/v3/gists/public",
   "rate_limit_url": "https://github.west.isilon.com/api/v3/rate_limit",
   "repository_url": "https://github.west.isilon.com/api/v3/repos/{owner}/{repo}",
   "repository_search_url": "https://github.west.isilon.com/api/v3/search/repositories?q={query}{&page,per_page,sort,order}",
   "current_user_repositories_url": "https://github.west.isilon.com/api/v3/user/repos{?type,page,per_page,sort}",
   "starred_url": "https://github.west.isilon.com/api/v3/user/starred{/owner}{/repo}",
   "starred_gists_url": "https://github.west.isilon.com/api/v3/gists/starred",
   "team_url": "https://github.west.isilon.com/api/v3/teams",
   "user_url": "https://github.west.isilon.com/api/v3/users/{user}",
   "user_organizations_url": "https://github.west.isilon.com/api/v3/user/orgs",
   "user_repositories_url": "https://github.west.isilon.com/api/v3/users/{user}/repos{?type,page,per_page,sort}",
   "user_search_url": "https://github.west.isilon.com/api/v3/search/users?q={query}{&page,per_page,sort,order}"
   }

   Then use the repository_search_url:
   xujian@EAT ~/tmp/fix $ curl -X GET -u xjian "https://github.west.isilon.com/api/v3/search/repositories?q=created:>2017-04-01" | grep -w ssh_url
   Enter host password for user 'xjian':
   "ssh_url": "git@github.west.isilon.com:Rancher/rancher-tutorial.git",
   "ssh_url": "git@github.west.isilon.com:cmeyer/jenkins-build-cli.git",
   "ssh_url": "git@github.west.isilon.com:eng-cee/isilon-cee.git",
   "ssh_url": "git@github.west.isilon.com:eng-cee/cee-tools.git",

** find all hard link files in current directory
   for i in $(find . -type f -links +1 2>/dev/null); do find -samefile $i 2>/dev/null | awk '{printf "%s ", $1}'; printf "\n"; done | sort | uniq
** get https cert info
   openssl s_client -showcerts -connect github.com:443 </dev/null 2>/dev/null|openssl x509 -outform PEM
** curl: (60) Peer's Certificate issuer is not recognized
   root@EAT /home/xujian/tmp # yum install ca-certificates
   Loaded plugins: fastestmirror
   Loading mirror speeds from cached hostfile
 * base: mirrors.cat.pdx.edu
 * epel: mirrors.lug.mtu.edu
 * extras: mirror.dal10.us.leaseweb.net
 * updates: mirrors.cat.pdx.edu
   Package ca-certificates-2018.2.22-70.0.el7_5.noarch already installed and latest version
   Nothing to do
   root@EAT /home/xujian/tmp # update-ca-trust force-enable
   root@EAT /home/xujian/tmp # ls /etc/pki/ca-trust/source/anchors/

   # get cert file from browser, such as chrome, then save as emc_root.cer
   # note to store all levels of CA, then for each level:
   root@EAT /home/xujian/tmp # openssl x509 -in emc_root.cer -outform pem -out emc_root.pem

   # or get pem text from openssl:
   openssl s_client -connect pypi.org:443 -showcerts

   # read the cert pem to verify:
   openssl x509 -in xxx.pem -noout -text

   root@EAT /home/xujian/tmp # cp xxx.pem /etc/pki/ca-trust/source/anchors/
   root@EAT /home/xujian/tmp # update-ca-trust extract
   
   # check timestamp to make sure updated:
   ls /etc/pki/tls/certs/ca-bundle.crt

   root@EAT /home/xujian/tmp # exit
** java cert ca problem
   # Jenkins fail to install some plugins because of this
   # the Linux Java cacerts dir:
   [root@base /]# ls -l ./etc/pki/java/cacerts
   lrwxrwxrwx. 1 root root 40 Jul  4 10:34 ./etc/pki/java/cacerts -> /etc/pki/ca-trust/extracted/java/cacerts
   [root@base /]# ls -l /etc/pki/ca-trust/extracted/java/cacerts
   -r--r--r--. 1 root root 160286 Jul  4 10:34 /etc/pki/ca-trust/extracted/java/cacerts
   [root@base /]# ls -l ./usr/lib/jvm/java-1.8.0-openjdk-1.8.0.181-3.b13.el7_5.x86_64/jre/lib/security/cacerts
   lrwxrwxrwx. 1 root root 41 Aug 16 13:21 ./usr/lib/jvm/java-1.8.0-openjdk-1.8.0.181-3.b13.el7_5.x86_64/jre/lib/security/cacerts -> ../../../../../../../etc/pki/java/cacerts
   [root@base /]# ls -l ./usr/lib/jvm/java-1.7.0-openjdk-1.7.0.191-2.6.15.4.el7_5.x86_64/jre/lib/security/cacerts
   lrwxrwxrwx. 1 root root 41 Aug 16 22:33 ./usr/lib/jvm/java-1.7.0-openjdk-1.7.0.191-2.6.15.4.el7_5.x86_64/jre/lib/security/cacerts -> ../../../../../../../etc/pki/java/cacerts

   # set trusted ca:
   openssl x509 -in <(openssl s_client -connect plugins.jenkins.io:443 -prexit 2>/dev/null) -out ~/example.crt
   keytool -importcert -file jenkins_plugin.crt -alias jenkins_plugin -keystore /etc/pki/java/cacerts -storepass changeit

   # for script for this
   #!/bin/bash
   key_name=$(echo $https_host_name| tr '.' '_').crt
   https_host_name=$1
   openssl s_client -connect $https_host_name:443 -prexit <<< quit

   echo
   echo
   echo "captured key $key_name:"

   openssl x509 -in <(openssl s_client -connect $https_host_name:443 -prexit 2>/dev/null) -out ~/$key_name && cat ~/$key_name &&
   keytool -importcert -file ~/$key_name -alias $(echo $https_host_name| tr '.' '_') -keystore /etc/pki/java/cacerts -storepass changeit


   # However can not find Linux ControlPanel to set this:
   -Dcom.sun.net.ssl.checkRevocation=false

** pip install fails with connection error: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:
   some yum libs may related to this problem:
     yum update python36-libs-3.6.8-1.el7.x86_64 (/usr/lib64/python3.6/ssl.py)
     yum ca-certificates
     yum update openssl

   But python may use its own cacert:
   some python libs urllib3, requests, certifi
    site-packages/certifi/cacert.pem
    site-packages/pip/_vendor/certifi/cacert.pem


   at last solved by global pip.conf:
   xujian@EAT ~/storage/notes $ cat /etc/pip.conf
   [global]
   trusted-host = pypi.python.org
                  pypi.org
                  files.pythonhosted.org
   and add these trusted-host to ~/.config/pip/pip.conf also.

   poetry has similar error, and solved by:

   ./lib/python3.6/site-packages/urllib3/connectionpool.py
        if False and not conn.is_verified:
           ^^^^^^^^^
            warnings.warn(
                (
                    "Unverified HTTPS request is being made. "
                    "Adding certificate verification is strongly advised. See: "
                    "https://urllib3.readthedocs.io/en/latest/advanced-usage.html"
                    "#ssl-warnings"
                ),
                InsecureRequestWarning,
            )

   ./lib/python3.6/site-packages/requests/sessions.py
        # xujian
        return {'verify': False, 'proxies': proxies, 'stream': stream,
                          ^^^^^
                'cert': cert}


** virt-manager corrupted font when X forwarding
   yum install dejavu-lgc-sans-fonts

** elasticsearch

*** commands
   # for indice "regression_wheel_dashboard", type "test_info":
     "_index": "regression_wheel_dashboard",
     "_type": "test_info",
     "_id": "AWOA9hcLc7O0-xfn3hBG",
     "_version": 2,

   # How to list elasticsearch indices
   curl -XGET "http://elasticsearch.elk.apps.isilon.com:9200/_cat/indices?v"

   # Run in KIBANA dev tools console
   # fix "Fielddata is disabled on text fields by default" issue, so the text fields can index document
   # this is aggres query:
   GET regression_wheel_dashboard/test_info/_search
   {
     "query": {
       "match_all": {}
     },
     "aggs": {
       "tests_by_branch": {
         "terms": {
           "field": "branch",
           "size": 10
         }
       }
     }
   }
   # to enable in 5.x
   PUT regression_wheel_dashboard/_mapping/test_info
   {
     "properties": {
       "branch": { 
         "type":     "text",
         "fielddata": true
       }
     }
   }
   # to enable in 6.x
   PUT regression_wheel_dashboard/_mapping/test_info
   {
     "properties": {
       "branch": { 
         "type":     "keyword",
       }
     }
   }
   
   # dump es mapping
   curl -XGET http://elasticsearch.elk.apps.isilon.com:9200/regression_wheel_dashboard/_mapping/test_info
   
   # search in URL
   http://elasticsearch.elk.apps.isilon.com:9200/regression_wheel_dashboard/test_info/_search
   
   # snapshot backup and restore
   GET /_snapshot
   
   PUT /_snapshot/my_fs_backup
   {
   "type": "fs",
   "settings": {
   "location": "/var/esbackup",
   "compress": true
   }
   }
   
   PUT /_snapshot/my_fs_backup/snapshot_1?wait_for_completion=true
   GET /_snapshot/my_fs_backup/snapshot_1
   
   GET rw_dashboard_test_info/test_info/_search
   {
   "query": {
   "match": {
   "resource": 307817
   }
   }
   }
   
   POST rw_dashboard_test_info/test_info/_delete_by_query
   {
   "query": {
   "match": {
   "resource": 307817
   }
   }
   }
   
   POST rw_dashboard_test_info/_close
   
   POST /_snapshot/my_fs_backup/snapshot_1/_restore
   {
   "indices": "rw_dashboard_test_info"
   }
   
   POST rw_dashboard_test_info/_open
   
*** script aggregation for multiple fields
GET commits/commits/_search
{
    "size": 0,
     "query": {
       "match_all": {}
     },
     "aggs": {
       "data": {
         "terms": {
                "size": 30,        
                "script": "doc['filename.keyword'].value+doc['span'].value"
          }
       }
     }
}

**** count for time related status
     POST rw_dashboard_test_info/test_info/_search
     {
       "size": 0, 
       "query":
       {
             "bool": {
                 "must": [
                     {
                         "range": {
                             "queue_time": {
                                 "lt": "2018-07-13T12:00:00"
                             }
                         }
                     },
                     {
                         "range": {
                             "start_time": {
                                 "gte": "2018-07-13T12:00:00"
                             }
                         }
                     }                    
                 ]
             }        
       }
     }
**** aggression with value range
     POST rw_dashboard_test_info/test_info/_search
     {
       "size": 0, 
       "query":
       {
         "match_all" : {}
       },
       "aggs":{
           "per_tt":{
               "date_histogram": {
                   "field": "start_time",
                   "interval": "day"
               },
               "aggs" : {
                 "pid_ranges" : {
                     "range" : {
                         "field" : "id",
                         "ranges" : [
                             { "to" : 100.0 },
                             { "from" : 100.0, "to" : 200.0 },
                             { "from" : 200.0 }
                         ]
                     }
                 }
               }
             }
         }
     }
     
**** get mapping
     GET rw_dashboard_test_info/_mapping/test_info

**** set mapping
     PUT rw_dashboard_test_info/_mapping/test_info
     {
       "properties": {
         "in_queued_seconds":{
           "type": "long"
         }
       }
     }
     

*** define filter aggregation for Kibana, e.g. filter "miss" and "match":
GET ita/ita/_search
{
  "size": 0,
  "aggs":{
    "stats":{
      "filters": {
        "filters": {
          "miss": {
            "term":{
              "testlist": "miss"
            }
          },
          "match": {
            "bool": {
              "must_not": {
                "term":{
                  "testlist": "miss"
                }
              }
            }
          }
        }
      }
    }
  }
}
*** cluster health state red since unassigned shards
    http://es-cluster-2.prod.rdu1.west.isilon.com:9200/_cluster/health?pretty
    curl -XGET http://es-cluster-1.prod.sea1.west.isilon.com:9200/_cat/shards?h=index,shards,state,prirep,unassigned.reason | grep UNASSIGNED
    curl -XDELETE "http://es-cluster-1.prod.sea1.west.isilon.com:9200/ac_testcase_results_822-2019-10-02"
    curl -XPOST 'http://es-cluster-1.prod.sea1.west.isilon.com:9200/_cluster/reroute?pretty' -H 'Content-Type: application/json' -d'{
        "commands" : [{
            "allocate_stale_primary" : {
            "index" : "ac_testcase_results_000-2021-02-04", "shard" : 0,"node" : "es-cluster-1", "accept_data_loss" : true}}]}'
** VM inside VMWARE VM (but the performance is pool)
   yum install virt-manager
   yum install qemu
   yum install libvirt
   yum install qemu-kvm qemu-kvm-common
   yum install seabios
   systemctl start libvirtd
   systemctl enable libvirtd

** xrdp + tigervnc
   https://www.itzgeek.com/how-tos/linux/centos-how-tos/install-xrdp-on-centos-7-rhel-7.html

   # installation
   # yum groupinstall "GNOME Desktop" "Graphical Administration Tools"  # "Graphical Administration Tools" seems not needed.
   yum -y install xrdp #tigervnc-server
   yum groupinstall "Server with GUI"

   systemctl start xrdp
   systemctl enable xrdp

   # allow all user to login with XRDP
   /etc/xrdp/sesman.ini
   ; TerminalServerUsers=tsusers
   ; TerminalServerAdmins=tsadmins

   # do not set DISPLAY in case XRDP
   if [ -z $XRDP_SESSION ];then
     export DISPLAY=$(who am i|cut -d'(' -f 2|cut -d')' -f 1):0.0
   fi

   # correct permissionn problem after first failure login
   chown xujian:xujian /run/user/1000/dconf

   # not needed since selinux disabled
   chcon --type=bin_t /usr/sbin/xrdp
   chcon --type=bin_t /usr/sbin/xrdp-sesman

   # gnome has performance issue: gnone-shell llvmpipe graphic rendering on CPU, not GPU.
   # so switch to KDE to fix:
   yum groupinstall 'KDE'
   
   xujian@EAT ~ $ ls -l ~/.Xclients
   -rwxr-xr-x 1 xujian xujian 9 Jul  4 11:51 /home/xujian/.Xclients
   xujian@EAT ~ $ cat .Xclients
   startkde

   # session control, in sesman.ini:
   [Sessions]
   ;; Policy - session allocation policy
   ; Type: enum [ "Default" | "UBD" | "UBI" | "UBC" | "UBDI" | "UBDC" ]
   ; Default: Xrdp:<User,BitPerPixel> and Xvnc:<User,BitPerPixel,DisplaySize>
   ; "UBD" session per <User,BitPerPixel,DisplaySize>
   ; "UBI" session per <User,BitPerPixel,IPAddr>
   ; "UBC" session per <User,BitPerPixel,Connection>
   ; "UBDI" session per <User,BitPerPixel,DisplaySize,IPAddr>
   ; "UBDC" session per <User,BitPerPixel,DisplaySize,Connection>
   Policy=Default

   # kill dead session
   ps -ef | grep ...
   ps -C Xvnc -o pid,user
   ps -C xrdp -o pid,user

** gnome settings
   # list settings
   gsettings list-keys org.gnome.Vino
   
   # enabe fraction scale, but seems not work
   gsettings set org.gnome.mutter experimental-features "['scale-monitor-framebuffer']"
   
   # security?
   gsettings set org.gnome.Vino require-encryption false

   # lock screen after 1 hour
   gsettings set org.gnome.desktop.session idle-delay 3600

** pip
*** pip use additional repo
   - State "TODO"       from ""           [2019-07-03 Wed 16:51]
   xujian@EAT ~/.config/pip $ cat /home/xujian/.config/pip/pip.conf
   [global]
   extra-index-url = http://artifactory.west.isilon.com:8081/artifactory/api/pypi/pypi-repo/simple
   trusted-host = artifactory.west.isilon.com

   As stated in the documentation, the default locations for Linux are:
   per-user: $HOME/.config/pip/pip.conf
   global: /etc/pip.conf
   export PIP_CONFIG_FILE = /path/to/pip.conf

   Can also define base repo:
   index-url = http://download.zope.org/ppix
   
*** pip do not use disutils
    pip install --trusted-host pypi.org --upgrade setuptools
** repaire corrupted python package
   easy_install -U urllib3
   this way is more powerful than pip install
   
** centos7 NFS client/server setup
   https://www.howtoforge.com/nfs-server-and-client-on-centos-7
   
** VS code for python virtualenv
   In VS user setting:
    {
        "git.ignoreLegacyWarning": true,
        "window.zoomLevel": 4,
        "editor.fontSize": 12,
        "editor.fontFamily": "'Monaco', 'Consolas', 'Droid Sans Mono', 'monospace', monospace, 'Droid Sans Fallback'",
        "http.proxyStrictSSL": false,
        "workbench.editor.enablePreview": false,
        "workbench.editor.enablePreviewFromQuickOpen": false,
        "python.linting.pylintPath": "/bin/pylint",
        "workbench.colorCustomizations": {
            "editor.selectionBackground": "#8a6b2f",
            "editor.selectionHighlightBackground": "#915696",
            "editorBracketMatch.background": "#8b6816",
            "editorBracketMatch.border": "#c11a1a"
        },
        "terminal.integrated.rendererType": "dom",
        "python.linting.pep8Enabled": true,
        "python.linting.pep8Path": "/bin/pep8",
        "files.exclude": {
            "**/.vscode/**": true,
            "**/*.diff": true
        },
        "window.titleBarStyle": "custom"
    }
    
   Setup virtualenv
    $ cat ~/bin/venv
    #!/bin/bash
    name=$1
    if [ "$name" == "" ];then
        "usage: venv <path>"
        exit
    fi
    
    # setup vscode workspace settings:
    mkdir -p $name/.vscode
    # full path is not needed:
    # "python.pythonPath": "\${workspaceFolder}/venv/bin/python"
    cat > $name/.vscode/settings.json <<EOF
    {
            "python.pythonPath": "venv/bin/python"
            "python.linting.pylintPath": "venv/bin/pylint"
    }
    EOF
    
    virtualenv $name/venv
    . $name/venv/bin/activate
    pip install pytest pylint

** elasticsearch/kibana diagrams

*** timelion
    .es(timefield=queue_time, q='(status:"Setup Failure") OR (status:"Did Not Launch")').label('Infra Error Trend').trend().lines(width=1), .es(timefield=queue_time, q='(status:"Setup Failure") OR (status:"Did Not Launch")').mvavg(3d).label('Infra Error'), .es(timefield=start_time, q=*).mvavg(3d).label('capacity'), .es(timefield=start_time, q=*).label('capacity trend').trend().lines(width=1)
    .es(timefield=start_time,q='supported_hw:"infinity"').divide(.es(timefield=start_time, q=*)).multiply(100).mvavg(3d).bars(stack=true).label('infinity (%)'),
    .es(timefield=start_time,q='NOT supported_hw:"infinity"').divide(.es(timefield=start_time, q=*)).multiply(100).mvavg(3d).bars(stack=true).label('gen4/5 (%)')
    .es(timefield=queue_time, q='(status:"Setup Failure") OR (status:"Did Not Launch")').label('Infra Error Trend').trend(), .es(timefield=queue_time, q='(status:"Setup Failure") OR (status:"Did Not Launch")').mvavg(3d).label('Infra Error')
    .es(timefield=start_time, q='in_queued_seconds:[0 TO 18000]').mvavg(3d).label('5 hours'), .es(timefield=start_time, q='in_queued_seconds:[0 TO 18000]').label('5 hours trend').trend().lines(width=1),
    .es(timefield=start_time, q='in_queued_seconds:[18000 TO 86400]').mvavg(3d).label('1 day'), .es(timefield=start_time, q='in_queued_seconds:[18000 TO 86400]').label('1 day trend').trend().lines(width=1),
    .es(timefield=start_time, q='in_queued_seconds:[86400 TO 8640000]').mvavg(3d).label('more days'), .es(timefield=start_time, q='in_queued_seconds:[86400 TO 8640000]').label('more days trend').trend().lines(width=1)

** set ntp and timezone
  643  yum install -y chrony
  644  cat > /etc/chrony.conf
  645  timedatectl set-timezone Etc/GMT
  646  systemctl enable chronyd
  647  systemctl start chronyd
  648  timedatectl
  649  date
** github
   # rebase with commit
   git remote add upstream git@github.west.isilon.com:IME-Patch/correlation.git
   git pull upstream master
  
   git fetch upstream
   git merge upstream/master

   # rebase without commit
   git remote add upstream git@github.west.isilon.com:IME-Patch/workflow_tools.git
   git fetch upstream
   git rebase upstream/master
** vagrant on centos 7
   # install packages
   curl -k https://releases.hashicorp.com/vagrant/2.1.2/vagrant_2.1.2_x86_64.rpm -o vagrant_2.1.2_x86_64.rpm
   rpm -i vagrant_2.1.2_x86_64.rpm
   # need to disable secure sign in repo config:
   wget http://download.virtualbox.org/virtualbox/rpm/rhel/virtualbox.repo
   yum install VirtualBox-5.2
   # install corresponding kernel head then setup virtualbox driver
   uname -r
   yum install kernel-devel-3.10.0-862.3.2.el7.x86_64
   yum install kernel-headers-3.10.0-862.3.2.el7.x86_64
   rcvboxdrv setup
   # disable secure vagrant box download
   config.vm.box_download_insecure = true
** k8s
*** install minikube
    https://kubernetes.io/docs/setup/learning-environment/minikube/

    solve docker driver error:
    https://stackoverflow.com/questions/43794169/docker-change-cgroup-driver-to-systemd
    failed to run Kubelet: failed to create kubelet: misconfiguration: kubelet cgroup driver: "cgroupfs" is different from docker cgroup driver: "systemd"

    configure minikube without vm:
    minikube start --vm-driver=none

*** build Jenkins container
    https://www.blazemeter.com/blog/how-to-setup-scalable-jenkins-on-top-of-a-kubernetes-cluster/

** Python profile
python -m cProfile -o run.cprof /storage/xujian/dev/tools/bz_mign/venv/bin/mg list --bug 273000
pyprof2calltree -k -i run.cprof

from ita_cc_data.cc_db import get_testlist_for_commits
g=GithubProject('isilon', 'onefs')
cProfile.run('get_testlist_for_commits(g, ["4bc065097a1f0cb725dd4ef5a1a3084cd61db719"])', 'run.cprof')
pyprof2calltree -k -i run.cprof

** modify psql auto seq
bzmign=> create sequence migrate_id_seq;
CREATE SEQUENCE
bzmign=> alter table migrate alter id set default nextval('migrate_id_seq');
ALTER TABLE
bzmign=> select setval('migrate_id_seq', 1950);
 setval 
--------
   1950
(1 row)
